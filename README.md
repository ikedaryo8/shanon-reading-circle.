# A Mathematical Theory of Communication 輪読会

# index
|No.|content|page|
|---|---|:---:|
|0|Introduction|1|
|PartI|Discrete noiseless systems|3|
|1|The discrete noiseless channel|3|
|2|The discrete source of information|4|
|3|The series of approximations to English|7|
|4|Graphical representation of a markoff process|8|
|5|Ergodic and mixed sources|8|
|6|Choice, Uncertainty and entropy|10|
|7|The Entropy of an informations source|13|
|8|Representation of the Encoding and Decoding operations|15|
|9|The Fundamental theoresm for a noiseless channel|16
|10|Discussion and Examples|17|
||||
|PARTII|The Discrete channel with noise|19|
|11|Representation of a noisy discrete channel|19|
|12|Equivocation and      channel capacity|20|
|13|The fundamental theorem for a discrete channel with noise|22|
|14|Discussion|24|
|15|Example of a discrete channel and its capacity|25|
|16|The channel capacity in certain special cases|26|
|17|An example of efficient coding|27|
|Appendix1|The growth of the number of blocks of symbols with a finite state condition|28|
|Appendix2|Derivation of $H = -\sum_{P_i}\log{P_i}$|28|
|Appendix3|Theorems on ergodic sources|29|
|Appendix4|Maximizing the rate for a system of constraints|30|
||||
|PARTIII|Mathematical preliminaries|32|
|18|Sets and ensembles of functions|32|
|19|Band limited ensembles of functions|34|
|20|Entropy of a continuous distribution|35|
|21|Entropy of an ensemble of functions|38|
|22|Entropy of loss in linear filters|39|
|23|Entropy of a sum of two ensembles|40|
||||
|PART IV|The continuous channel|41|
|24|the capacity of a continuous channel|41|
|25|Channel capacity with an average power limitation|43|
|26|The cannel capacity with a peak power limitation|45|
||||
|PART V|The rate for a continuous source|47|
|27|Fidelity evaluation functions|47|
|28|The rate for source relative to fidelity evaluation|49|
|29|The calculation of rates|50|
|Appendix5||52|
|Appendix6||52|
|Appendix7||54|
|Appendix6||52|
